# -*- coding: utf-8 -*-
"""Telecom Churn .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SUQI2aRU5LvAYRI1ORs19RTWWwEVaoqz
"""

# Commented out IPython magic to ensure Python compatibility.
# Loading the libraries

import pandas as pd
import numpy as np
import seaborn as sns
# %matplotlib inline
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

import warnings
warnings.filterwarnings('ignore')

Data=pd.read_csv('/content/telecom.csv')# Importing the dataframe
pd.set_option('display.max_columns',25)# this helps to view every column
Data.head()# to view rows





# Removing CustomerID as it is of no use
Data.drop('customerID',axis=1,inplace=True)

# rows and columns the data has

Data.shape

# To see the basic data types of each column

Data.info()

""" # From the above we can see that total charges is dispalyed as objects whereas it is a integer.

# So we are converting it to a numeric column.
"""

Data.TotalCharges = pd.to_numeric(Data.TotalCharges, errors='coerce')

# Checking whethere there is a null value in the data

Data.isnull().sum()

"""We can see that total charges has eleven NaN values.
Since we have around 7000 records we can remove the null values.
"""

# Removing the null values

Data.dropna(inplace=True)

# Converting the String churn column in to binary by replacing Yes with 1 and No with 0

Data['Churn'].replace("Yes",1,inplace=True)
Data['Churn'].replace("No",0,inplace=True)

# Converting the categorical columns in to binary using get dummies

Data1=pd.get_dummies(Data)
Data1.head()

Data1.replace({True: 1, False: 0})

plt.figure(figsize=(15,6))
Data1.corr()['Churn'].sort_values(ascending=False).plot.bar()

"""We can see that month to month contract and absence of Online Security and Tech support is strongly positively correlated with the traget variable Churn and we also got the same info while doing visual anlaysis of the data in Tableau.

Also we can see that Tenure and Two Year contract is negatively correlated with target variable Churn.

With basic Visual Analytics done in Tableau and Basic Exploration done here we are heading to model building and evaluation.
"""

Data1['Churn'].value_counts().plot(kind='barh',figsize=(8,6))
plt.xlabel("Count")
plt.ylabel("Target Variable")
plt.title("Count of TARGET Variable")

Data1['Churn'].value_counts()

"""# here 0= Not churn and 1= Churned
# but these numbers are not making sense lets make a sense out of it  
"""

len(Data1['Churn'])

100*Data1['Churn'].value_counts()/len(Data1['Churn'])

"""Here we can infer that only approx 26% of customers churn where as 73% of customers continue with hteir isp"""

Data1.isnull().sum()

"""no null values great!!

# lets plot every column with respect to churn
## this is part of data exploration
"""

for i, predictor in enumerate(Data1.drop(columns=['Churn','TotalCharges','MonthlyCharges'])):
    plt.figure(i)
    sns.countplot(x=predictor,data=Data1,hue='Churn')

"""# Checking relationship between totalcharges and monthly charges"""

sns.lmplot(x='MonthlyCharges',y='TotalCharges',data=Data1,fit_reg=False)

"""# Model building
#

# Validation Dataset

We will use 70% of the dataset for modeling and hold back 30% for Test/Validation.
"""

# Split-out validation dataset
x = Data1.drop('Churn',axis=1)
y = Data1['Churn']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=42)

"""Balancing the data set using smote  for imbalance handling"""

smote=SMOTE()
x_smote, y_smote = smote.fit_resample(x_train,y_train)
print("unbalanced dataset shape", len(y_train))
print("after SMOTE resampling", len(y_smote))

# KNN model
from sklearn.metrics import accuracy_score
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train, y_train)
y_pred = knn.predict(x_test)
print('Accuracy score:', accuracy_score(y_test, y_pred))

# Logistic Regression model
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(x_train, y_train)
y_pred = log_reg.predict(x_test)
print('Accuracy score (Logistic Regression):', accuracy_score(y_test, y_pred))

# Train the SVM model-LINEAR FUNCTION
from sklearn.metrics import classification_report,confusion_matrix
svm = SVC(kernel='linear')
svm.fit(x_train, y_train)
y_pred = svm.predict(x_test)
print('Accuracy score (SVM):', accuracy_score(y_test, y_pred))
print('Classification report (SVM):', classification_report(y_test, y_pred))
print('Confusion matrix (SVM):', confusion_matrix(y_test, y_pred))

# SVM model SIGMOID FUNCTON
svm = SVC(kernel='sigmoid')
svm.fit(x_train, y_train)
y_pred = svm.predict(x_test)
print('Accuracy score (SVM- SIGMOID):', accuracy_score(y_test, y_pred))
print('Classification report (SVM- SIGMOID):', classification_report(y_test, y_pred))
print('Confusion matrix (SVM- SIGMOID):', confusion_matrix(y_test, y_pred))

# DECESION TREE MODEL
decesiontree = DecisionTreeClassifier(random_state=42)
decesiontree.fit(x_train, y_train)
y_pred = decesiontree.predict(x_test)
print('Accuracy score (DECESION TREE):', accuracy_score(y_test, y_pred))
print('Classification report (DECESION TREE):', classification_report(y_test, y_pred))
print('Confusion matrix (DECESION TREE):', confusion_matrix(y_test, y_pred))

#  Gaussian Naive Bayes classifier
gnb = GaussianNB()
gnb.fit(x_train, y_train)
y_pred = gnb.predict(x_test)
print('Accuracy score (Gaussian Naive Bayes):', accuracy_score(y_test, y_pred))
print('Classification report (Gaussian Naive Bayes):', classification_report(y_test, y_pred))
print('Confusion matrix (Gaussian Naive Bayes):', confusion_matrix(y_test, y_pred))

"""FROM ABOVE RESULTS WE CAN SEE THAT ONLY 3 MODELS ARE PERFORMING WELL I.E LOGISTIC  REGRESSION, KNN, AND SVM(WITH LINEAR FUNCTION)"""

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42, replacement=True)

x_rus, y_rus = rus.fit_resample(x, y)

print('BEFORE RUS:', len(y))
print('AFTER RUS:', len(y_rus))

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(sampling_strategy='minority')

x_ros, y_ros = ros.fit_resample(x, y)

print('Original dataset shape:', len(y))
print('Resample dataset shape:', len(y_ros))

#  Logistic Regression model with RandomOverSampler
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(x_ros, y_ros)

y_pred = log_reg.predict(x_test)
print('Accuracy score (Logistic Regression with RandomOverSampler):', accuracy_score(y_test, y_pred))
print('Classification report (Logistic Regression with RandomOverSampler):', classification_report(y_test, y_pred))
print('Confusion matrix (Logistic Regression with RandomOverSampler):', confusion_matrix(y_test, y_pred))

#  KNN model with RandomOverSampler
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_ros, y_ros)
y_pred = knn.predict(x_test)
print('Accuracy score (KNN with RandomOverSampler):', accuracy_score(y_test, y_pred))
print('Classification report (KNN with RandomOverSampler):', classification_report(y_test, y_pred))
print('Confusion matrix (KNN with RandomOverSampler):', confusion_matrix(y_test, y_pred))

# SVM model with RandomOverSampler
from sklearn.svm import SVC
svm = SVC(kernel='linear')
svm.fit(x_ros, y_ros)

y_pred = svm.predict(x_test)

print('Accuracy score (SVM with RandomOverSampler):',accuracy_score(y_ros, y_pred))
print('Classification report (SVM with RandomOverSampler):', classification_report(y_test, y_pred))
print('Confusion matrix (SVM with RandomOverSampler):', confusion_matrix(y_test, y_pred))

# Logistic Regression with RandomOverSampler
log_reg = LogisticRegression(max_iter=1000)
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}
grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_ros, y_ros)
print('Best parameters lOGISTIC REGRESSION RANDOMOVERSAMPLER:', grid_search.best_params_)
print('Best score lOGISTIC REGRESSION RANDOMOVERSAMPLER:', grid_search.best_score_)
# KNN with RandomOverSampler
knn = KNeighborsClassifier()
param_grid = {'n_neighbors': range(1, 21)}
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_ros, y_ros)
print('Best parameters n_neighbors:', grid_search.best_params_)
print('Best score n_neighbors:', grid_search.best_score_)
# SVM with RandomOverSampler
#svm = SVC(kernel='linear')
#param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}
#grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')
#grid_search.fit(x_ros, y_ros)
#print('Best parameters GridSearchCV:', grid_search.best_params_)
#print('Best score GridSearchCV:', grid_search.best_score_)
# Logistic Regression with RandomUnderSampler
log_reg = LogisticRegression(max_iter=1000)
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}
grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_rus, y_rus)
print('Best parameters Logistic Regression with RandomUnderSampler:', grid_search.best_params_)
print('Best score Logistic Regression with RandomUnderSampler:', grid_search.best_score_)
# KNN with RandomUnderSampler
knn = KNeighborsClassifier()
param_grid = {'n_neighbors': range(1, 21)}
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_rus, y_rus)
print('Best parameters KNN with RandomUnderSampler:', grid_search.best_params_)
print('Best score KNN with RandomUnderSampler:', grid_search.best_score_)
# SVM with RandomUnderSampler
#svm = SVC(kernel='linear')
#param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}
#grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')
#grid_search.fit(x_rus, y_rus)
#print('Best parameters SVM with RandomUnderSampler:', grid_search.best_params_)
#print('Best score SVM with RandomUnderSampler:', grid_search.best_score_)
# Logistic Regression with unbalanced data
log_reg = LogisticRegression(max_iter=1000)
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}
grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_train, y_train)
print('Best parameters Logistic Regression with unbalanced data:', grid_search.best_params_)
print('Best score Logistic Regression with unbalanced data:', grid_search.best_score_)
# KNN with unbalanced data
knn = KNeighborsClassifier()
param_grid = {'n_neighbors': range(1, 21)}
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_train, y_train)
print('Best parameters KNN with unbalanced data:', grid_search.best_params_)
print('Best score KNN with unbalanced data:', grid_search.best_score_)
# SVM with unbalanced data
#svm = SVC(kernel='linear')
#param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}
#grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')
#grid_search.fit(x_train, y_train)
#print('Best parameters:', grid_search.best_params_)
#print('Best score:', grid_search.best_score_)





"""# Evaluating Baseline Models

We are going to make simple kfold cross validation accross models and see what works the best for the data.
"""

# Test options and evaluation metric
num_folds = 10
scoring = 'accuracy'

"""Let's create a baseline of performance on this problem and spot-check a number of different algorithms. We will select a suite of different algorithms capable of working on this classification problem. The algorithms selected include:
1. Linear Algorithms: Logistic Regression (LR)
2. Nonlinear Algorithms: Classiffication and Regression Trees (CART),k-Nearest Neighbors (KNN),Naive Bayes and Support Vector Classifier (SVC)

We suspect that the differing scales of the raw data may be negatively impacting the skill of some of the algorithms.

Let's evaluate the  algorithms with a standardized copy of the dataset.

This is where the data is transformed such that each attribute has a mean value of zero and a standard deviation of 1.

We also need to avoid data leakage when we transform the data.

A good way to avoid leakage is to use pipelines that standardize the data and build the model for each fold in the cross-validation test harness.

That way we can get a fair estimation of how each model with standardized data might perform on unseen data.
"""

# Standardize the dataset
pipelines = []
pipelines.append(('LR', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])))
pipelines.append(('KNN', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsClassifier())])))
pipelines.append(('CART', Pipeline([('Scaler', StandardScaler()), ('CART', DecisionTreeClassifier())])))
pipelines.append(('NB', Pipeline([('Scaler', StandardScaler()), ('NB', GaussianNB())])))
pipelines.append(('SVC', Pipeline([('Scaler', StandardScaler()), ('SVC', SVC())])))

results = []
names = []
for name, model in pipelines:
    kfold = KFold(n_splits=num_folds, random_state=42, shuffle=True)
    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

# Compare Algorithms
fig = plt.figure(figsize=(6,6))
fig.suptitle('Scaled Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()

"""Logistic Regression and SVC has the best accuracy with minimum standard deviation.

Lets see whethere we can improve the same with tuning.

# Improve Results With Tuning
"""

# SVC tuning
scaler = StandardScaler().fit(x_train)
scaledx = scaler.transform(x_train)
scaledtest=scaler.transform(x_test)

c_values = np.arange(1,5)
kernel=['linear','rbf']
param_grid = dict(C=c_values,kernel=kernel)
model_svc=SVC()
kfold = KFold(n_splits=num_folds, random_state=42, shuffle=True)
grid_svc = GridSearchCV(estimator=model_svc, param_grid=param_grid, scoring=scoring, cv=kfold)
grid_result_svc = grid_svc.fit(scaledx, y_train)

grid_result_svc.score(scaledx,y_train)

"""With Tuning the accuracy of SVC is increased by 2%.

Now we will check the test score.
"""

grid_result_svc.score(scaledtest,y_test)

"""The model is almost a perfect fit."""

# DecesionTreee Tuning

scaler = StandardScaler().fit(x_train)
scaledx = scaler.transform(x_train)
scaledtest=scaler.transform(x_test)

max_depth= np.arange(1,8)
max_features=np.arange(1,45)
min_samples_split=np.arange(2,6)
param_grid = dict(max_depth=max_depth,max_features=max_features,min_samples_split=min_samples_split)
model_dt=DecisionTreeClassifier()
kfold = KFold(n_splits=num_folds, random_state=42, shuffle=True)
grid_dt = GridSearchCV(estimator=model_dt, param_grid=param_grid, scoring=scoring, cv=kfold)
grid_result_dt = grid_dt.fit(scaledx, y_train)

grid_result_dt.score(scaledx,y_train)

"""The accuracy is improved by 8 % but still it is lower than SVC.

We will check the test score.
"""

grid_result_dt.score(scaledtest,y_test)

"""There is a slight over fit."""

# Logistic regression Tuning

scaler = StandardScaler().fit(x_train)
scaledx = scaler.transform(x_train)
scaledtest=scaler.transform(x_test)

c_values = np.arange(2**-5,2**5)
param_grid = dict(C=c_values)
model_lr=LogisticRegression()
kfold = KFold(n_splits=num_folds, random_state=42, shuffle=True)
grid_lr = GridSearchCV(estimator=model_lr, param_grid=param_grid, scoring=scoring, cv=kfold)
grid_result_lr = grid_lr.fit(scaledx, y_train)

grid_result_lr.score(scaledx,y_train)

"""There isnt a much improvement in the score.

We wil check the test score.
"""

grid_result_lr.score(scaledtest,y_test)

"""We can see that the model is great with perfect fit.

# Ensemble Methods

Another way that we can improve the performance of algorithms on this problem is by using ensemble methods.

We will evaluate two different ensemble machine learning algorithms, one boosting and one bagging method:

1. Boosting Methods: AdaBoost (AB),GradientBoost (GB)
2. Bagging Methods: Random Forests (RF), Bagging (BG)
"""

# ensembles
ensembles = []
ensembles.append(('AB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostClassifier())])))
ensembles.append(('GB', Pipeline([('Scaler', StandardScaler()),('GB', GradientBoostingClassifier())])))
ensembles.append(('RF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestClassifier())])))
ensembles.append(('Bag', Pipeline([('Scaler', StandardScaler()),('Bag',BaggingClassifier())])))

results = []
names = []
for name, model in ensembles:
    kfold = KFold(n_splits=num_folds, random_state=42,shuffle=True)
    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

# Compare Algorithms
fig = plt.figure(figsize=(6,6))
fig.suptitle('Scaled Ensemble Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()

"""Adaboost and GradientBoost ensemble gives the best results.

We will try imporvingg the score further by tuning.
"""

# Adaboost Tuning

scaler = StandardScaler().fit(x_train)
scaledx = scaler.transform(x_train)
scaledtest=scaler.transform(x_test)
base_estimator=[LogisticRegression(C=6.03125, class_weight=None, dual=False,
                fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)]
n_estimators=np.arange(1,51)
param_grid = dict(base_estimator=base_estimator,n_estimators=n_estimators)
model_ab=AdaBoostClassifier()
kfold = KFold(n_splits=num_folds, random_state=42,shuffle=True)
grid_ab = GridSearchCV(estimator=model_ab, param_grid=param_grid, scoring=scoring, cv=kfold)
grid_result_ab = grid_ab.fit(scaledx, y_train)

grid_ab.score(scaledx,y_train)

"""There is a slight increase of 0.4 %."""

grid_ab.score(scaledtest,y_test)

"""Model is almost perfect fit."""

# GradientBoost Tuning

scaler = StandardScaler().fit(x_train)
scaledx = scaler.transform(x_train)
scaledtest=scaler.transform(x_test)


n_estimators=np.arange(1,101)
param_grid = dict(n_estimators=n_estimators)
model_gb=GradientBoostingClassifier()
kfold = KFold(n_splits=num_folds, random_state=42,shuffle=True)
grid_gb = GridSearchCV(estimator=model_ab, param_grid=param_grid, scoring=scoring, cv=kfold)
grid_result_gb = grid_gb.fit(scaledx, y_train)

grid_gb.score(scaledx, y_train)

"""There is 0.1% increase in the accuracy."""

grid_gb.score(scaledtest, y_test)

"""Model is at the right fit."""

results=pd.DataFrame(index=['Logistic Regression',
                            'Decision Tree','Support Vector Classifier',
                            'Adaboost Classifier','Gradientboost Classifier'])

results['Train']=[grid_result_lr.score(scaledx,y_train),
                  grid_result_dt.score(scaledx,y_train),
                  grid_result_svc.score(scaledx,y_train),
                  grid_result_ab.score(scaledx,y_train),
                  grid_result_gb.score(scaledx,y_train)]

results['Test']=[grid_result_lr.score(scaledtest,y_test),
                  grid_result_dt.score(scaledtest,y_test),
                  grid_result_svc.score(scaledtest,y_test),
                  grid_result_ab.score(scaledtest,y_test),
                  grid_result_gb.score(scaledtest,y_test)]

results

